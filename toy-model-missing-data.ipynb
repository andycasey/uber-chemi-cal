{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import stan_utils as stan\n",
    "from mpl_utils import (mpl_style, common_limits)\n",
    "\n",
    "plt.style.use(mpl_style)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(N, M, D, overlap_fraction=None, scales=None,\n",
    "                  phi_scale=1, seed=None):\n",
    "    \"\"\"\n",
    "    Generate some toy data to play with. Here we assume all :math:`N` stars have\n",
    "    been observed by all :math:`M` surveys.\n",
    "\n",
    "    :param N:\n",
    "        The number of stars observed.\n",
    "\n",
    "    :param M:\n",
    "        The number of surveys.\n",
    "\n",
    "    :param D:\n",
    "        The dimensionality of the label space.\n",
    "\n",
    "    :param overlap_fraction: [optional]\n",
    "        The approximate fraction of stars observed by each survey. If `None` is\n",
    "        provided then some random ~50% of stars will be observed by each survey.\n",
    "        If provided, this should be a list of length `M` with values between 0\n",
    "        (no overlap) and 1 (complete overlap).\n",
    "\n",
    "    :param scales: [optional]\n",
    "        Optional values to provide for the relative scales on the latent factors.\n",
    "\n",
    "    :param seed: [optional]\n",
    "        An optional seed to provide to the random number generator.\n",
    "        \n",
    "    :param phi_scale: [optional]\n",
    "        An optional scale value for the uncertainties.\n",
    "\n",
    "    :returns:\n",
    "        A two-length tuple containing the data :math:`y` and a dictionary with\n",
    "        the true values.\n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if scales is None:\n",
    "        scales = np.abs(np.random.normal(0, 1, size=D))\n",
    "\n",
    "    else:\n",
    "        scales = np.array(scales)\n",
    "        \n",
    "    if overlap_fraction is None:\n",
    "        overlap_fraction = np.random.normal(0.5, 0.05, size=M)\n",
    "    else:\n",
    "        overlap_fraction = np.array(overlap_fraction)\n",
    "        assert  overlap_fraction.size == M\n",
    "        \n",
    "\n",
    "    assert len(scales) == D\n",
    "\n",
    "    X = np.random.normal(\n",
    "        np.zeros(D),\n",
    "        scales,\n",
    "        size=(N, D))\n",
    "\n",
    "    # TODO: Better way to randomly generate positive semi-definite covariance\n",
    "    #       matrices that are *very* close to an identity matrix.\n",
    "\n",
    "    # Use Cholesky decomposition to ensure the resulting covariance matrix is \n",
    "    # positive definite.\n",
    "    L = np.random.randn(M, D, D)\n",
    "    L[:, np.arange(D), np.arange(D)] = np.exp(L[:, np.arange(D), np.arange(D)])\n",
    "    i, j = np.triu_indices_from(L[0], 1)\n",
    "    L[:, i, j] = 0.0\n",
    "\n",
    "    theta = np.array([np.dot(L[i], L[i].T) for i in range(M)])\n",
    "\n",
    "    y = np.dot(X, theta)\n",
    "    for m, of in enumerate(overlap_fraction):\n",
    "        indices = np.where(np.random.uniform(0, 1, size=N) <= of)[0]\n",
    "        y[indices, m, :] = np.nan\n",
    "        \n",
    "    # add noise.\n",
    "    phi = np.abs(np.random.normal(0, phi_scale, size=(M, D)))\n",
    "    rank = np.random.normal(0, 1, size=y.shape)\n",
    "\n",
    "    noise = scales * rank * phi \n",
    "\n",
    "    y += noise\n",
    "    \n",
    "    \n",
    "\n",
    "    truths = dict(X=X, theta=theta, phi=phi, scales=scales, L=L, noise=noise)\n",
    "\n",
    "    return (y, truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, M, D = (250, 10, 5)\n",
    "scales = np.ones(D)\n",
    "phi_scale = 1\n",
    "\n",
    "y, truths = generate_data(N=N, M=M, D=D, \n",
    "                          scales=scales, \n",
    "                          phi_scale=phi_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an additive variance array to account for missing data.\n",
    "def de_gapify_data(y, additional_variance=1e8):\n",
    "    \"\"\"\n",
    "    Fill NaNs in the label vectors with the mean values of other\n",
    "    labels. No structure is assumed for the missing labels (e.g.,\n",
    "    some labels could be missing from some surveys, but not all).\n",
    "    \n",
    "    :param y:\n",
    "        The label vector, where NaNs represent missing data.\n",
    "    \n",
    "    :param additional_variance: [optional]\n",
    "        The variance to add for missing data.\n",
    "    \"\"\"\n",
    "    \n",
    "    N, M, D = y.shape\n",
    "    missing = ~np.isfinite(y)\n",
    "    mean_labels = np.nanmean(y, axis=1)\n",
    "    \n",
    "    y_full_rank = np.copy(y)\n",
    "    variance = np.zeros_like(y)\n",
    "    for m in range(M):\n",
    "        y_full_rank[:, m, :][missing[:, m, :]] = mean_labels[missing[:, m, :]]\n",
    "        variance[:, m, :][missing[:, m, :]] = 1.0\n",
    "    \n",
    "    variance *= additional_variance\n",
    "    \n",
    "    return (y_full_rank, variance)\n",
    "\n",
    "y_full_rank, extra_variance = de_gapify_data(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_ca6366cf3cdb64f5dbb7e41b4cb69f3a NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/*\n",
      "  Latent factor model for chemical abundances from multiple studies, allowing\n",
      "  for missing data.\n",
      "*/\n",
      "\n",
      "data {\n",
      "  int<lower=1> N; // number of stars\n",
      "  int<lower=1> D; // dimensionality of the data (number of labels)\n",
      "  int<lower=1> M; // number of surveys (or studies)\n",
      "  vector[D] y[N, M]; // the labels as reported by various surveys.\n",
      "  vector<lower=0>[D] scales; // fixed relative scales for latent factors\n",
      "  vector[D] y_additive_variance[N, M]; // variance to add for missing data.\n",
      "}\n",
      "\n",
      "transformed data {\n",
      "  vector[D] mu; // the mean of the data in each dimension\n",
      "  int<lower=1> Q; // the number of non-zero lower-triangular entries that we\n",
      "                  // need for the decomposoition of our theta matrix\n",
      "  Q = M * choose(D, 2);\n",
      "\n",
      "  // TODO: Stop assuming that the user is not an idiot\n",
      "  mu = rep_vector(0.0, D);\n",
      "}\n",
      "\n",
      "parameters {\n",
      "  vector[D] X[N]; // latent factors for each star\n",
      "  vector<lower=0>[M] phi[D]; // variance on survey labels\n",
      "\n",
      "  vector[Q] L_lower_triangular; // lower triangular entries of the decomposition\n",
      "                                // of the  theta matrix\n",
      "  vector<lower=0>[M] L_diag[D]; // diagonal entries of the decomposition of the \n",
      "                                // theta matrix\n",
      "}\n",
      "\n",
      "transformed parameters {\n",
      "  cholesky_factor_cov[D, D] L[M];\n",
      "  matrix[D, D] theta[M];\n",
      "  {\n",
      "    int q = 0;\n",
      "\n",
      "    for (m in 1:M)\n",
      "      for (i in 1:D)\n",
      "        for (j in (i + 1):D) \n",
      "          L[m, i, j] = 0.0;\n",
      "\n",
      "    for (m in 1:M) {\n",
      "      for (i in 1:D) {\n",
      "        L[m, i, i] = L_diag[i, m];\n",
      "        for (j in (i + 1):D) {\n",
      "          q = q + 1;\n",
      "          L[m, j, i] = L_lower_triangular[q];\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "\n",
      "    for (m in 1:M)\n",
      "      theta[m] = multiply_lower_tri_self_transpose(L[m]);\n",
      "  }\n",
      "}\n",
      "\n",
      "model {\n",
      "\n",
      "  // Place priors on various properties.\n",
      "  for (d in 1:D) {\n",
      "    X[:, d] ~ normal(rep_vector(0, N), rep_vector(scales[d], N));\n",
      "    phi[d] ~ normal(rep_vector(0, M), rep_vector(1, M));\n",
      "\n",
      "    // TODO: Should we be placing an inverted Wishart prior or something on this\n",
      "    //       shit?\n",
      "    //L_lower_triangular\n",
      "\n",
      "    // TODO: revisit this prior\n",
      "    L_diag[d] ~ normal(rep_vector(1, M), rep_vector(0.1, M));\n",
      "  }\n",
      "\n",
      "  for (n in 1:N) \n",
      "    for (m in 1:M)\n",
      "      y[n, m, :] ~ normal(to_row_vector(X[n]) * theta[m], \n",
      "                          sqrt(to_vector(phi[:, m]) + y_additive_variance[n, m, :]));\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = stan.read_model(\"model-missing-data.stan\")\n",
    "print(model.model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model\n",
    "data = dict(N=N, M=M, D=D, scales=scales, \n",
    "            y=y_full_rank, y_additive_variance=extra_variance)\n",
    "\n",
    "op_kwds = dict(\n",
    "    data=data, \n",
    "    iter=100000, \n",
    "    tol_obj=7./3 - 4./3 - 1, # machine precision\n",
    "    tol_grad=7./3 - 4./3 - 1, # machine precision\n",
    "    tol_rel_grad=1e3,\n",
    "    tol_rel_obj=1e4\n",
    ")\n",
    "\n",
    "p_opt = model.optimizing(**op_kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 10, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, D, figsize=(4 * D, 4))\n",
    "for d, ax in enumerate(axes):\n",
    "\n",
    "    x = truths[\"X\"].T[d]\n",
    "    y = p_opt[\"X\"].T[d]\n",
    "\n",
    "    ax.scatter(x, y)\n",
    "\n",
    "    ax.set_xlabel(r\"$X_{{{0},\\textrm{{true}}}}$\".format(d))\n",
    "    ax.set_ylabel(r\"$X_{{{0},\\textrm{{opt}}}}$\".format(d))\n",
    "\n",
    "    common_limits(ax, plot_one_to_one=True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
